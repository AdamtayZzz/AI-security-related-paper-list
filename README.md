*This repo records the papers I have read throughout my undergraduate period*

*My current research interests are in AI Security*

# Reinforcement Learning

## Design

### Basic 

- 2015 Nature [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)

### Multi-Agents 

- 2016 ICML [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)

## Adversarial Attack

- 2017 ICLR Workshop [Adversarial Attacks on Neural Network Policies](https://arxiv.org/abs/1702.02284)
- 2017 MLDM [Vulnerability of Deep Reinforcement Learning toPolicy Induction Attacks](https://arxiv.org/abs/1701.04143)
- 2017 IJCAI [Tactics of Adversarial Attack on Deep Reinforcement Learning Agents](https://arxiv.org/abs/1703.06748)
- 2018 CSCS [Sequential Attacks on Agents for Long-Term Adversarial Goals](https://arxiv.org/abs/1805.12487)
- 2020 S&P Workshop [On the Robustness of Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2003.03722.pdf)
- 2020 ICML [Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning](https://arxiv.org/abs/2003.12909)

## Others

- 2017 ICML [Robust Adversarial Reinforcement Learning](https://arxiv.org/abs/1703.02702)

# Adversarial Attack

## Attack

- 2014 ICLR [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199)
- 2015 ICLR [Explaining and harnessing adversarial examples](https://arxiv.org/abs/1412.6572)
- 2016 S&P [The Limitations of Deep Learning in Adversarial Settings](https://arxiv.org/abs/1511.07528)
- 2016 CVRP [DeepFool: a simple and accurate method to fool deep neural networks](https://arxiv.org/abs/1511.04599)
- 2017 ICLR [Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533)
- 2017 CCS [Practical Black-Box Attacks against Machine Learning](https://arxiv.org/abs/1602.02697)
- 2018 AAAI [Adversarial Transformation Networks: Learning to Generate Adversarial Examples](https://arxiv.org/abs/1703.09387)
- 2018 NDSS [TextBugger: Generating Adversarial Text Against Real-world Applications](https://arxiv.org/abs/1812.05271)
- 2019 S&P [Intriguing Properties of Adversarial ML Attacks in the Problem Space](https://arxiv.org/abs/1911.02142)

## Defense

- 2016 S&P [Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks](https://arxiv.org/abs/1511.04508)
- 2017 S&P [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644)

# Backdoor Attack

## Attack

- 2017 Arxiv [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)
- 2018 NDSS [Trojaning Attack on Neural Networks](https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&context=cstech)

## Defense

- 2018 RAID [Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks](https://arxiv.org/abs/1805.12185) 
- 2019 S&P [Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks](https://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf)
- 2019 CCS [ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation](https://dl.acm.org/doi/10.1145/3319535.3363216)

# Data Poisoning

- 2017 ICML [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730)
- 2018 NeurIPS [Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/abs/1804.00792)

# Model Inference

## Data Reconstruction

- 2019 NeurIPS [Deep Leakage from Gradients](https://papers.nips.cc/paper/9617-deep-leakage-from-gradients.pdf)
- 2020 USENIX Security[Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning](https://arxiv.org/abs/1904.01067)

## Membership Inference Attack

- 2017 S&P [Membership Inference Attacks against Machine Learning Models](https://arxiv.org/abs/1610.05820)

## Model Inversion

- 2015 CCS [Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures](https://dl.acm.org/doi/10.1145/2810103.2813677)

## Property Inference Attack

- 2018 CCS [Property Inference Attacks on Fully Connected Neural Networksusing Permutation Invariant Representations](https://dl.acm.org/doi/10.1145/3243734.3243834)

# Federated Learning

