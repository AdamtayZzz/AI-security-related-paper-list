*This repo records the papers I have read throughout my undergraduate period*

*My current research interests are in AI Security*

# Reinforcement Learning

## Design

### Basic 

- 2015 Nature [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)

### Multi-Agents 

- 2016 ICML [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)

## Adversarial Attack

- 2017 ICLR Workshop [Adversarial Attacks on Neural Network Policies](https://arxiv.org/abs/1702.02284)
- 2017 IJCAI [Tactics of Adversarial Attack on Deep Reinforcement Learning Agents](https://arxiv.org/abs/1703.06748)
- 2020 S&P Workshop [On the Robustness of Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2003.03722.pdf)

## Others

- 2017 ICML [Robust Adversarial Reinforcement Learning](https://arxiv.org/abs/1703.02702)

# Adversarial Attack

## Attack

- 2014 ICLR [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199)
- 2015 ICLR [Explaining and harnessing adversarial examples](https://arxiv.org/abs/1412.6572)
- 2016 CVRP [DeepFool: a simple and accurate method to fool deep neural networks](https://arxiv.org/abs/1511.04599)
- 2017 ICLR [Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533)
- 2017 S&P [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644)
- 2018 NDSS [TextBugger: Generating Adversarial Text Against Real-world Applications](https://arxiv.org/abs/1812.05271)
- 2019 S&P [Intriguing Properties of Adversarial ML Attacks in the Problem Space](https://arxiv.org/abs/1911.02142)

## Defense

- 2016 S&P [Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks](https://arxiv.org/abs/1511.04508)

# Backdoor Attack



# Data Poisoning



# Model Inference



# Federated Learning

